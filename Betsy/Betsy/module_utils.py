"""
Objects:
IdentifiedDataNode

Functions:

# Network and Nodes
get_inputid
high_light_path     Does something to network.

# Network
download_ftp
download_dataset   # move to GEO?

# Zip
gunzip
extract_from_zip
unzip_if_zip

# File operations
merge_two_files

# Matrix operations
is_missing        If a matrix has missing values.  Move?
format_convert    Transpose matrix?
replace_matrix_header
convert_gene_list_platform
convert_to_same_platform

# Outputs
write_Betsy_report_parameters_file
plot_line_keywds
plot_line_keywd
find_pcaplots

# Miscellaneous
is_number
renew_parameters    Delete a list of keys from a dictionary.

find_fastq_files         Find all fastq files in a folder.
find_merged_fastq_files  Find the fastq generated by merge_reads.
find_fasta_files         Find all fasta files in a folder.

find_picard_jar

read_sample_group_file
fix_sample_group_filenames
assert_sample_group_file

check_inpath
make_filename

calc_max_procs_from_ram

"""
# _find_ids_that_pass_filters


#FMT = "%a %b %d %H:%M:%S %Y"

# DataNode + identifier.
class IdentifiedDataNode:
    def __init__(self, data, identifier=""):
        self.data = data
        self.identifier = identifier
    def __repr__(self):
        x = str(self.data) + ' identifier:' + self.identifier
        return x


## class AntecedentFilter:
##     def __init__(self, datatype_name=None, contents=None, **attributes):
##         self.datatype_name = datatype_name
##         self.contents = contents
##         self.attributes = attributes
##         self.mismatch_reason = None
##     def matches_node(self, node):
##         if self.datatype_name and self.datatype_name != node.datatype.name:
##             self.mismatch_reason = "datatype_name"
##             return False
##         if self.contents and node.attributes.get("contents") != self.contents:
##             self.mismatch_reason = "contents"
##             return False
##         for (key, value) in self.attributes.iteritems():
##             if node.attributes.get(key) != value:
##                 self.mismatch_reason = "attributes: %s" % key
##                 return False
##         self.mismatch_reason = None
##         return True
##     def __str__(self):
##         return self.__repr__()
##     def __repr__(self):
##         x = [self.datatype_name, repr(self.contents)]
##         for name, value in self.attributes.iteritems():
##             # Bug: strings should be hashed.
##             x.append("%s=%s" % (name, value))
##         x = "%s(%s)" % (self.__class__.__name__, ", ".join(x))
##         return x


## def _find_ids_that_pass_filters(network, node_ids, filters):
##     # For each of the filters, pick out one of the nodes.  Return a
##     # list of nodes parallel to filters, or None if not found

##     matches = []
##     for f in filters:
##         for id_ in node_ids:
##             if id_ in matches:  # don't reuse nodes
##                 continue
##             if f.matches_node(network.nodes[id_]):
##                 matches.append(id_)
##                 break
##         else:
##             return None
##     assert len(matches) == len(filters)
##     return matches


## def find_antecedents(network, module_id, user_attributes, pool, *filters):
##     # filters should be AntecedentFilter objects.  Return either a
##     # IdentifiedDataNode (if 0 or 1 filters), or a list of
##     # IdentifiedDataNodes parallel to filters.  Raises an exception if
##     # no antecedents could be found.
##     import os
##     import bie3

##     module_name = network.nodes[module_id].name

##     # Make a list of every possible combination of inputs that goes
##     # into this module.
##     prev_ids = []
##     for id_ in network.transitions:
##         if module_id in network.transitions[id_]:
##             prev_ids.append(id_)
##     all_input_ids = bie3._get_valid_input_combinations(
##         network, module_id, prev_ids, user_attributes)

##     # Filter for just the combinations in which all input nodes have
##     # been run.
##     filtered = []
##     for input_ids in all_input_ids:
##         # If not all the input nodes have been run, then ignore this
##         # combination.
##         x = [x for x in input_ids if x in pool]
##         if len(x) != len(input_ids):
##             continue
##         filtered.append(x)
##     all_input_ids = filtered

##     # Filter based on the user criteria.
##     ids = None
##     if not filters:
##         # If no filters given, then just return the first id found.
##         assert all_input_ids
##         assert all_input_ids[0]
##         ids = [all_input_ids[0][0]]
##     else:
##         for input_ids in all_input_ids:
##             ids = _find_ids_that_pass_filters(network, input_ids, filters)
##             if ids is not None:
##                 break
##     assert ids, "antecedents not found: %s" % module_name
##     for id_ in ids:
##         if not pool[id_].identifier:
##             continue
##         assert os.path.exists(pool[id_].identifier), (
##             "File not found: %s" % pool[id_].identifier)
##     objs = [pool[x] for x in ids]
##     assert not filters or len(objs) == len(filters)
##     # Return a single IdentifiedDataNode if 0 or 1 filters given.
##     if len(objs) <= 1:
##         objs = objs[0]
##     return objs


## def get_identifier(
##     network, module_id, pool, user_attributes, datatype=None, contents=None,
##     optional_key=None, optional_value=None, second_key=None, second_value=None,
##     **param):
##     # Returns a single IdentifiedDataNode that goes into this module.  What is
##     # this used for?
##     import os
##     import bie3

##     assert not (optional_key and not optional_value)
##     assert not (optional_value and not optional_key)
##     assert not (second_key and not second_value)
##     assert not (second_value and not second_key)

##     # Make a list of every possible combination of inputs that goes
##     # into this module.
##     prev_ids = []
##     for id_ in network.transitions:
##         if module_id in network.transitions[id_]:
##             prev_ids.append(id_)
##     all_input_ids = bie3._get_valid_input_combinations(
##         network, module_id, prev_ids, user_attributes)

##     # Filter for just the all_input_ids in which all input nodes have
##     # been run.
##     filtered = []
##     for input_ids in all_input_ids:
##         # If not all the input nodes have been run, then ignore this
##         # combination.
##         x = [x for x in input_ids if x in pool]
##         if len(x) != len(input_ids):
##             continue
##         filtered.append(x)
##     all_input_ids = filtered

##     # Filter based on the user criteria.
##     ids = []
##     for input_ids in all_input_ids:
##         for id_ in input_ids:
##             if id_ in ids:
##                 continue
##             node = network.nodes[id_]
##             if datatype and node.datatype.name != datatype:
##                 continue
##             if contents and node.attributes.get("contents") != contents:
##                 continue
##             if optional_key and \
##                    node.attributes.get(optional_key) != optional_value:
##                 continue
##             if second_key and \
##                    node.attributes.get(second_key) != second_value:
##                 continue
##             all_found = True
##             for key in param:
##                 if node.attributes.get(key) != param[key]:
##                     all_found = False
##             if not all_found:
##                 continue
##             ids.append(id_)

##     assert ids, 'cannot find node that match for %s' % \
##                network.nodes[module_id].name)
##     for id_ in ids:
##         if pool[i].identifier:
##             assert os.path.exists(pool[i].identifier), (
##                 'the input file %s for %s does not exist' %
##                 (pool[i].identifier, network.nodes[module_id].name))
##     id_ = ids[0]
##     return pool[id_]


# Not sure exactly what this does.  Looks like it takes some sort of
# file identifier, and returns some sort of cleaned up id.
def get_inputid(identifier):
    import os

    # identifier is in format:
    # <data type>_<file>.<ext>
    # class_label_<user_given_name>.cls
    # Pull out just the <user_given_name>.
    x = identifier
    x = os.path.split(x)[-1]     # get file (no path)
    x = os.path.splitext(x)[-2]  # get base name (no extension)
    x = x.split("_")[-1]         # ???
    return x
    #old_filename = os.path.split(identifier)[-1]
    #old_filename_no_ext = os.path.splitext(old_filename)[-2]
    #inputid = old_filename_no_ext.split('_')[-1]
    #return inputid


# Why is this here?  And why is rma hard coded?
def find_pcaplots(network, pool, module_id, rma=False):
    import os

    before_pcaplot = None
    after_pcaplot = None
    for x in pool:
        node, node_id = pool[x], x
        if not node.data.datatype.name == 'PcaPlot':
            continue
        if module_id in network.transitions[node_id]:
            assert os.path.exists(node.identifier), (
                'the input file %s for %s does not exist' %
                (node.identifier, network.nodes[module_id].name))
            if (node.data.attributes['quantile_norm'] == 'no' and
                node.data.attributes['combat_norm'] == 'no' and
                node.data.attributes['shiftscale_norm'] == 'no' and
                node.data.attributes['bfrm_norm'] == 'no' and
                node.data.attributes['dwd_norm'] == 'no' and
                node.data.attributes['gene_center'] == 'no' and
                node.data.attributes['gene_normalize'] == 'no' and
                node.data.attributes['unique_genes'] == 'no' and
                node.data.attributes['platform'] == 'no' and
                node.data.attributes['duplicate_probe'] == 'no' and
                node.data.attributes['group_fc'] == 'no'):
                before_pcaplot = node
            else:
                after_pcaplot = node
        if not after_pcaplot:
            after_pcaplot = before_pcaplot
    return before_pcaplot, after_pcaplot


# Why is this here?  Should also be named has_missing_values?
def is_missing(identifier):
    import arrayio

    M = arrayio.read(identifier)
    has_missing = False
    for i in range(M.dim()[0]):
        for j in range(M.dim()[1]):
            if M._X[i][j] is None:
                has_missing = True
                break
        if has_missing:
            break
    return has_missing


def merge_two_files(A_file, B_file, handle):
    """input two files and merge, write the output to handle"""
    import arrayio
    from genomicode import Matrix
    from genomicode import matrixlib

    M_A = arrayio.read(A_file)
    M_B = arrayio.read(B_file)
    assert arrayio.tab_delimited_format.is_matrix(M_A)
    assert arrayio.tab_delimited_format.is_matrix(M_B)
    [M_A, M_B] = matrixlib.align_rows(M_A, M_B)
    assert M_A.nrow() > 0, 'there is no common genes between two files'
    X = []
    for i in range(M_A.dim()[0]):
        x = M_A._X[i] + M_B._X[i]
        X.append(x)
    row_names = M_A._row_names
    row_order = M_A._row_order
    col_names = {}
    for name in M_A._col_names:
        if name not in M_B._col_names:
            continue
        newsample_list = []
        for sample in M_B._col_names[name]:
            if sample in M_A._col_names[name]:
                newsample = sample + '_2'
            else:
                newsample = sample
            newsample_list.append(newsample)
        #x = M_A._col_names[name] + M_B._col_names[name]
        x = M_A._col_names[name] + newsample_list
        col_names[name] = x
    M_c = Matrix.InMemoryMatrix(X, row_names, col_names, row_order)
    arrayio.tab_delimited_format.write(M_c, handle)


def format_convert(X):
    data = []
    for i in range(X.dim()[1]):
        data.append(X.value(None, i))
    return data


def write_Betsy_report_parameters_file(
    inputs, outfile, starttime, user, job_name):
    import os
    import json
    import time
    #from stat import *

    st = os.stat(outfile)
    modified_time = time.asctime(time.localtime(st[time.ST_MTIME]))
    f = file(os.path.join(os.getcwd(), 'Betsy_parameters.txt'), 'w')
    if isinstance(inputs, list):
        text = ['Module input:', [
            ('', i) for i in inputs
        ], 'Module output:', os.path.split(outfile)[-1],
                'Module output parameters:', '', 'Pipeline module sequence:',
                '', 'Start time:', starttime, 'Finish time:', modified_time,
                'User:', user, 'Jobname:', job_name]
    else:
        text = ['Module input:', ('', inputs), 'Module output:',
                os.path.split(outfile)[-1], 'Module output parameters:', '',
                'Pipeline module sequence:', '', 'Start time:', starttime,
                'Finish time:', modified_time, 'User:', user, 'Jobname:',
                job_name]
    newtext = json.dumps(text, sort_keys=True, indent=4)
    f.write(newtext)
    f.close()


def plot_line_keywds(filename, keywords, outfile):
    import arrayio
    from genomicode import mplgraph
    from genomicode import filelib

    M = arrayio.read(filename)
    header = M.row_names()
    label = M._col_names['_SAMPLE_NAME']
    outfiles = []
    for keyword in keywords:
        out = keyword + '.png'
        lines = []
        data = []
        legend_name = []
        for i in range(M.dim()[0]):
            if M.row_names(header[1])[i] == keyword:
                data.append(M.slice()[i])
                legend_name.append(M.row_names(header[0])[i])
        assert len(data) > 0, 'cannot find the keywords %s in the file %s' % (
            keywords, filename)
        for i in range(len(data)):
            line = [(j, data[i][j]) for j in range(len(data[i]))]
            lines.append(line)
        params = {
            "box_label" : label,
            "legend" : legend_name,
            "ylim_min" : 0,
            "ylabel" : keyword,
            "left" : 0.1,
            }
        fig = mplgraph.lineplot(*lines, **params)
        fig.savefig(out)
        outfiles.append(out)
    import Image
    img_w_list = []
    img_h_list = []
    imgs = []
    for i in range(len(outfiles)):
        img = Image.open(outfiles[i], 'r')
        img_w, img_h = img.size
        img_w_list.append(img_w)
        img_h_list.append(img_h)
        imgs.append(img)
    total_w = max(img_w_list) + 30
    total_h = sum(img_h_list) + 10
    background = Image.new('RGBA', (total_w, total_h), (255, 255, 255, 255))
    bg_w, bg_h = background.size
    offset_w = (bg_w - max(img_w_list)) / 2
    offset_h_list = []
    for i in range(len(img_h_list)):
        offset_h = bg_h - sum(img_h_list[i:])
        offset_h_list.append(offset_h)
    for img, offset_h in zip(imgs, offset_h_list):
        background.paste(img, (offset_w, offset_h))
    background.save(outfile)
    assert filelib.exists_nz(outfile), 'the plot_line_keywds fails'


def plot_line_keywd(filename, keyword, outfile):
    import arrayio
    from genomicode import mplgraph
    from genomicode import filelib

    M = arrayio.read(filename)
    header = M.row_names()
    label = M._col_names['_SAMPLE_NAME']
    lines = []
    data = []
    legend_name = []
    for i in range(M.dim()[0]):
        if M.row_names(header[1])[i] == keyword:
            data.append(M.slice()[i])
            x = "%s (%s)" % (keyword, M.row_names(header[0])[i])
            legend_name.append(x)
    assert len(data) > 0, 'cannot find the keyword %s in the file %s' % (
        keyword, filename)
    for i in range(len(data)):
        line = [(j, data[i][j]) for j in range(len(data[i]))]
        lines.append(line)
    params = {
        "box_label" : label,
        "legend" : legend_name,
        "ylim_min" : 0,
        "ylabel" : "Signal",
        "left" : 0.1,
        }
    fig = mplgraph.lineplot(*lines, **params)
    fig.savefig(outfile)
    assert filelib.exists_nz(outfile), 'the plot_line_keywd fails'


def renew_parameters(parameters, key_list):
    newparameters = parameters.copy()
    for key in key_list:
        if key in newparameters.keys():
            del newparameters[key]
    return newparameters


def is_number(s):
    try:
        float(s)
    except ValueError:
        return False
    return True


def download_ftp(host, path, filename):
    import ftplib
    from ftplib import FTP
    import socket
    try:
        ftp = FTP(host)
    except (socket.error, socket.gaierror), e:
        raise AssertionError('Error [%s]: cannot reach %s' % (str(e), host))
    try:
        ftp.login()
    except ftplib.error_perm, e:
        ftp.quit()
        raise AssertionError('Error [%s] :cannot login anonymously' % str(e))
    try:
        ftp.cwd(path)
    except ftplib.error_perm, x:
        if str(x).find('No such file') >= 0:
            raise AssertionError('cannot find the %s' % path)
    filelist = []  # to store all files
    ftp.retrlines('NLST', filelist.append)
    if filename in filelist:
        f = open(filename, 'wb')
        ftp.retrbinary('RETR ' + filename, f.write)
        f.close()
        ftp.close()
    else:
        ftp.close()
        raise AssertionError('cannot find %s in %s' % (filename, host))


def download_dataset(GSEID):
    import os
    import tarfile

    #download the tar folder from geo
    host = 'ftp.ncbi.nih.gov'
    GSE_directory = 'pub/geo/DATA/supplementary/series/' + GSEID
    download_rarfile = GSEID + '_RAW.tar'
    download_ftp(host, GSE_directory, download_rarfile)
    #untar the data folder
    GSEID_path = GSEID
    if not tarfile.is_tarfile(download_rarfile):
        raise ValueError('download file is not tar file')
    tar = tarfile.open(download_rarfile)
    tar.extractall(path=GSEID_path)
    tar.close()
    os.remove(download_rarfile)
    assert os.path.exists(GSEID_path), 'the download file %s\
                        does not exist' % GSEID_path
    assert len(os.listdir(GSEID_path)) > 0, 'the untar in \
           download_geo_dataset_GPL %s fails' % GSEID_path
    return GSEID_path


def gunzip(filename):
    import os
    import gzip
    import userfile

    x = userfile._unhash_storefile(filename)
    real_name = x
    if isinstance(x, tuple):
        real_name = x[1]
    if filename.endswith('.gz') or real_name.endswith('.gz'):
        newfilename = os.path.join(
            os.getcwd(), os.path.split(os.path.splitext(filename)[0])[-1])
        #unzip the gz data
        fileObj = gzip.GzipFile(filename, 'rb')
        fileObjOut = file(newfilename, 'wb')
        while 1:
            line = fileObj.readline()
            if line == '':
                break
            fileObjOut.write(line)
        fileObj.close()
        fileObjOut.close()
        assert os.path.exists(newfilename), (
            'unzip the file %s fails' % filename
        )
        return newfilename
    else:
        return None


def high_light_path(network_file, pipeline, out_file):
    from xml.dom.minidom import parseString

    pipeline1 = ['start']
    pipeline1.extend(pipeline)
    f = open(network_file, 'r')
    data = f.read()
    f.close()
    dom = parseString(data)
    nodes = dom.getElementsByTagName('node')
    edges = dom.getElementsByTagName('edge')
    for analysis in pipeline1:
        for node in nodes:
            value = node.childNodes[1].getAttributeNode('value').nodeValue
            if value == analysis:
                node.childNodes[11].attributes['fill'] = '#FF33CC'
    for i in range(len(pipeline1[:-1])):
        label = pipeline1[i] + ' (pp) ' + pipeline1[i + 1]
        for edge in edges:
            if edge.childNodes[1].getAttributeNode('value').nodeValue == label:
                edge.childNodes[7].attributes['width'] = '6'
    xmlstr = dom.toxml('utf-8')
    f = open(out_file, 'w')
    f.write(xmlstr)
    f.close()


def convert_gene_list_platform(genes, platform):
    from genomicode import jmath
    from genomicode import arrayplatformlib

    platform_list = [i.name for i in arrayplatformlib.platforms]
    assert platform in platform_list, (
        'we cannot convert to the platform %s' % platform
    )
    chip = arrayplatformlib.guess_chip_from_probesets(genes)
    assert chip, 'we cannot guess the platform for the input file'
    in_attribute = arrayplatformlib.get_bm_attribute(chip)
    in_mart = arrayplatformlib.get_bm_organism(chip)
    out_attribute = arrayplatformlib.get_bm_attribute(platform)
    out_mart = arrayplatformlib.get_bm_organism(platform)
    R = jmath.start_R()
    jmath.R_equals_vector(genes, 'gene_id')
    R('library(biomaRt)')
    jmath.R_equals(in_attribute, 'in_attribute')
    jmath.R_equals(in_attribute, 'filters')
    jmath.R_equals(in_mart, 'in_mart')
    R('old=useMart("ensembl",in_mart)')
    jmath.R_equals(out_attribute, 'out_attribute')
    jmath.R_equals(out_mart, 'out_mart')
    R('new=useMart("ensembl",out_mart)')
    R(str('homolog = getLDS(attributes=in_attribute,') +
      str('filters=filters,values=gene_id,mart=old,') +
      str('attributesL=out_attribute,martL=new)'))
    homolog = R['homolog']
    #old_id = [str(i) for i in homolog[0]]
    human_id = [str(i) for i in homolog[1]]
    return human_id


def convert_to_same_platform(filename1, filename2, platform=None):
    import arrayio
    import subprocess
    from genomicode import config
    from genomicode import arrayplatformlib
    from genomicode import filelib

    M1 = arrayio.read(filename1)
    platform1 = arrayplatformlib.identify_platform_of_matrix(M1)
    M2 = arrayio.read(filename2)
    platform2 = arrayplatformlib.identify_platform_of_matrix(M2)
    if platform1 == platform2:
        return filename1, filename2

    Annot_path = config.annotate_matrix
    Annot_BIN = filelib.which(Annot_path)
    assert Annot_BIN, 'cannot find the %s' % Annot_path
    if platform1 == platform:
        filename = filename2
        newfilename1 = filename1
        newfilename2 = 'tmp'
    elif platform2 == platform:
        filename = filename1
        newfilename1 = 'tmp'
        newfilename2 = filename2
        
    if platform:
        command = [
            'python', Annot_BIN, '-f', filename, '-o', 'tmp',
            "--platform", platform]
        process = subprocess.Popen(
            command, shell=False, stdout=subprocess.PIPE,
            stderr=subprocess.PIPE)
        error_message = process.communicate()[1]
        if error_message:
            raise ValueError(error_message)
        #assert module_utils.exists_nz('tmp'), (
        #    'the platform conversion fails')
        assert filelib.exists_nz('tmp'), 'the platform conversion fails'
    return newfilename1, newfilename2


    ##def extract_from_zip(zipName):
    ##    z = zipfile.ZipFile(zipName)
    ##    for f in z.namelist():
    ##        if f.endswith('/'):
    ##            os.makedirs(f)
    ##        else:
    ##            z.extract(f)


def extract_from_zip(zipName, outdir):
    import zipfile

    x = zipfile.ZipFile(zipName)
    x.extractall(path=outdir)


def unzip_if_zip(input_name):
    import os
    import zipfile

    if zipfile.is_zipfile(input_name):
        directory = os.path.split(input_name)[-1]
        directory = os.path.splitext(directory)[0]
        directory = os.path.join(os.getcwd(), directory)
        extract_from_zip(input_name, directory)
        for dirname in os.listdir(directory):
            if not dirname == '__MACOSX':
                directory = os.path.join(directory, dirname)
    else:
        directory = input_name
    return directory


## def replace_matrix_header(M, old_header, new_header):
##     # Actually converts from GCT to TDF format.
##     M = M.matrix()
##     assert old_header in M._row_order
##     M._row_names[new_header] = M._row_names[old_header]
##     del M._row_names[old_header]
##     ids = M._row_order
##     ids = [
##         x.replace(old_header, new_header) if x == old_header else x
##         for x in ids]
##     M._row_order = ids
##     return M


## def process_group_info(group_file):
##     """return a dict with <sample_name:[[left_sample_list],
##                                         [right_sample_list]]"""
##     f = file(group_file, 'r')
##     text = f.readlines()
##     f.close()
##     group_dict = {}
##     text = [line.strip() for line in text if line.strip()]
##     for line in text:
##         words = line.split('\t')
##         if len(words) == 3:
##             if words[0] not in group_dict:
##                 group_dict[words[0]] = [words[2]]
##             else:
##                 group_dict[words[0]].append(words[2])
##         elif len(words) == 4:
##             if words[0] not in group_dict:
##                 group_dict[words[0]] = [[words[2]], [words[3]]]
##             else:
##                 group_dict[words[0]][0].append(words[2])
##                 group_dict[words[0]][1].append(words[3])
##         else:
##             raise ValueError('group file is invalid')
##     return group_dict

##def concatenate_files(input_files,outfile):
##    with open(outfile,'w') as outfile:
##        for fname in input_files:
##            with gzip.open(fname) as infile:
##                for line in infile:
##                    outfile.write(line)
##                    
##def concatenate_multiple_line(group_dict,foldername):
##    current_dir = os.getcwd()
##    new_group_dict = {}
##    for sample_name, files in group_dict.iteritems():
##        if len(files)==1:
##            outfile = os.path.join(current_dir,sample_name+'.fastq')
##            inputfiles = [os.path.join(foldername,x) for x in files[0]]
##            concatenate_files(inputfiles,outfile)
##            new_group_dict[sample_name]=[outfile]
##        elif len(files)==2:
##            outfile_left = os.path.join(current_dir,sample_name+'_R1.fastq')
##            outfile_right = os.path.join(current_dir,sample_name+'_R2.fastq')
##            inputfiles_left = [os.path.join(foldername,x) for x in files[0]]
##            inputfiles_right = [os.path.join(foldername,x) for x in files[1]]
##    	    concatenate_files(inputfiles_left,outfile_left)
##            concatenate_files(inputfiles_right,outfile_right)
##            new_group_dict[sample_name] = [outfile_left,outfile_right]
##    return new_group_dict
##   
##           
##a=process_group_info('/home/xchen/NGS/try_RSEM/big_sample_data/sample_group.txt')
##b=concatenate_multiple_line(a,'/home/xchen/NGS/try_RSEM/big_sample_data/')
##print b


def find_fastq_files(path):
    # Return a list of the FASTQ files (full filenames) under path.
    import os
    from genomicode import filelib

    fastq_suffix = [".fq", ".fastq"]
    compress_suffix = [".gz", ".bz2", ".xz", ".zip"]
    
    all_suffixes = []
    all_suffixes.extend(fastq_suffix)
    for s1 in fastq_suffix:
        for s2 in compress_suffix:
            x = s1+s2
            all_suffixes.append(x)
    
    filenames = filelib.list_files_in_path(path)
    # Filter out the fastq files.
    i = 0
    while i < len(filenames):
        p, f = os.path.split(filenames[i])
        f_l = f.lower()
        if f.startswith("."):
            del filenames[i]
            continue
        found = False
        for s in all_suffixes:
            if f_l.endswith(s):
                found = True
                break
        if found:
            i += 1
        else:
            del filenames[i]
    return filenames


def find_fasta_files(path):
    # Return a list of the FASTA files (full filenames) under path.
    import os
    from genomicode import filelib

    fa_ext = [".fa", ".fasta"]
    compress_ext = ["", ".gz", ".xz", ".bz2"]
    EXTENSIONS = []
    for x1 in fa_ext:
        for x2 in compress_ext:
            x = "%s%s" % (x1, x2)
            EXTENSIONS.append(x)
    
    filenames = filelib.list_files_in_path(path)
    # Filter out the fastq files.
    i = 0
    while i < len(filenames):
        p, f = os.path.split(filenames[i])
        f_l = f.lower()
        if f.startswith("."):
            del filenames[i]
            continue
        for x in EXTENSIONS:
            if f_l.endswith(x):
                i += 1  # is fasta file
                break
        else:
            del filenames[i]
    return filenames


def find_bam_files(path):
    # Return a list of the .bam files (full filenames) under path.
    import os
    from genomicode import filelib
    
    filenames = filelib.list_files_in_path(path)
    # Filter out the bam files.
    i = 0
    while i < len(filenames):
        p, f = os.path.split(filenames[i])
        f_l = f.lower()
        if f.startswith("."):
            del filenames[i]
        elif f_l.endswith(".bam"):
            i += 1
        else:
            del filenames[i]
    return filenames


def find_merged_fastq_files(sample_group_filename, fastq_path):
    # Read the sample group file.  Return a list of (sample,
    # pair1.fastq, pair2.fastq).  pair2.fastq will be None for single
    # end reads.  Both files are full paths.
    import os
    
    sample_groups = read_sample_group_file(sample_group_filename)
    x = [x[1] for x in sample_groups]
    x = sorted({}.fromkeys(x))
    all_samples = x

    # Find the fastq files for each of the samples.
    # The merge_reads module will save fastq files in the format:
    # <fastq_path>/<sample>.fastq          # if single end
    # <fastq_path>/<sample>_<Pair>.fastq   # if paired end

    fastq_files = []
    for sample in all_samples:
        # Look for single or paired end fastq files.
        pair1 = pair2 = None
        se_file = os.path.join(fastq_path, "%s.fastq" % sample)
        pe_file1 = os.path.join(fastq_path, "%s_1.fastq" % sample)
        pe_file2 = os.path.join(fastq_path, "%s_2.fastq" % sample)
        if os.path.exists(se_file):
            pair1 = se_file
            assert not os.path.exists(pe_file1)
            assert not os.path.exists(pe_file2)
        elif os.path.exists(pe_file1):
            assert not os.path.exists(se_file)
            assert os.path.exists(pe_file2)
            pair1 = pe_file1
            pair2 = pe_file2
        else:
            raise AssertionError, "Not found: %s" % sample

        x = sample, pair1, pair2
        fastq_files.append(x)

    # Make sure all samples are unique.
    x1 = [x[0] for x in fastq_files]
    x2 = {}.fromkeys(x1).keys()
    assert len(x1) == len(x2), "dup sample"
        
    return fastq_files


def read_sample_group_file(file_or_handle):
    # Return list of (filename, sample, pair).  pair is None, 1, or 2.
    # 
    # Reads can be split across multiple files (e.g. for multiple
    # lanes), or across pairs.
    # Headers:
    # Filename  Sample  Pair
    # F1         A       1
    # F3         A       2
    # F2         A       1
    # F4         A       2
    # F5         B       1
    # F6         B       2
    #
    # - Filenames should be unique.
    # - Pair should be 1 or 2.  If single end reads, just leave blank.
    # - There can be many Filenames per Sample.  There can be many
    #   Pairs per Sample (if the reads for one pair are split).
    # - The pairs that match (1 to its 2 partner) should be next to
    #   each other in the file.
    import os
    from genomicode import filelib
    
    handle = file_or_handle
    if type(handle) is type(""):
        assert os.path.exists(file_or_handle)
        handle = filelib.openfh(handle)
        
    data = []
    for d in filelib.read_row(handle, header=1, pad_cols=""):
        pair = d.Pair.strip()
        assert pair in ["", "1", "2"], "Invalid pair: %s" % d.Pair
        x = d.Filename, d.Sample, pair
        data.append(x)
        
    # Make sure filenames are unique.
    seen = {}
    for x in data:
        filename, sample, pair = x
        assert filename not in seen, "Filenames not unique: %s" % filename
        seen[filename] = 1

    # For each sample, make sure there isn't a mix of paired and
    # single ended files.  It must be all single ended or all paired.
    x = [x[1] for x in data]
    all_samples = sorted({}.fromkeys(x))
    for sample in all_samples:
        x = [x[2] for x in data if x[1] == sample]
        x = sorted({}.fromkeys(x))
        if x == [""]:  # All single
            continue
        elif x == ["1", "2"]:  # All paired
            continue
        raise AssertionError, "Weird pairing: %s" % sample

    # Make sure each pair is next to each other.
    for sample in all_samples:
        pairs = [x[2] for x in data if x[1] == sample]
        # Should be all "", or a pattern of "1", "2".
        x = {}.fromkeys(pairs).keys()
        if x == [""]:  # all ""
            continue
        assert len(x) % 2 == 0, "Weird pairing: %s" % sample
        for i in range(0, len(x), 2):
            assert x[i] == "1", "Weird pairing: %s" % sample
            assert x[i+1] == "2", "Weird pairing: %s" % sample
        
    return data


def fix_sample_group_filenames(sample_groups, fastq_path):
    # Sometimes the file names can change, due to compression or
    # uncompression.  Try to adjust for this.
    import os

    x = os.listdir(fastq_path)
    fastq_files = {}.fromkeys(x)

    EXTENSIONS = [".gz", ".bz2", ".xz"]

    for i in range(len(sample_groups)):
        file_, sample, pair = sample_groups[i]

        # Try the file itself.
        to_try = [file_]
        # If the file ends with any of these extensions, then try the
        # uncompressed file.
        filestem = None
        for ext in EXTENSIONS:
            if file_.lower().endswith(ext):
                filestem = file_[:-len(ext)]
                to_try.append(filestem)
                break
        # Try the file with a different extension.
        if filestem:
            for ext in EXTENSIONS:
                x = filestem + ext
                to_try.append(x)
        # Try the file with these extensions.
        for ext in EXTENSIONS:
            x = file_ + ext
            to_try.append(x)
        for test_file in to_try:
            if test_file in fastq_files:
                break
        else:
            # File not found.
            continue
        
        x = test_file, sample, pair
        sample_groups[i] = x
    return sample_groups
    

def assert_sample_group_file(filename, fastq_path):
    import os
    
    x = read_sample_group_file(filename)
    x = fix_sample_group_filenames(x, fastq_path)
    sample_groups = x

    # Make sure each file can be found in the fastq folder.
    for x in sample_groups:
        file_, sample, pair = x
        filename = os.path.join(fastq_path, file_)
        assert os.path.exists(filename), "Missing FASTQ file: %s" % file_

    # Make sure there are no duplicate files.
    files = [x[0] for x in sample_groups]
    file2count = {}
    for x in files:
        file2count[x] = file2count.get(x, 0) + 1
    dups = sorted([x for x in file2count if file2count[x] > 1])
    assert not dups, "Duplicate files"
    # Make sure there are no FASTQ files without samples.
    # BROKEN. Does not account for compression, e.g. .gz.
    #all_filenames = [x[0] for x in data]
    #files = os.listdir(fastq_path)
    #for file in files:
    #    assert file in all_filenames, "Not in sample group file: %s" % file


def check_inpath(path):
    import os
    assert os.path.exists(path)
    assert os.path.isdir(path)
    return path
    

def get_user_option(
    user_options, name, not_empty=False, allowed_values=None, type=None,
    check_file=False):
    # not_empty means I will make sure the value is not an empty value.
    # required means the user must supply a value (even if default given).
    # allowed_values is a list of the allowed values of this option.
    # type should be a function that converts the type.
    import os
    
    assert name in user_options, "Missing option: %s" % name
    value = user_options[name]
    if not_empty:
        assert value, "Empty user option: %s" % name
    if allowed_values:
        assert value in allowed_values, "Invalid option for %s: %s" % (
            name, value)
    if value and type is not None:
        value = type(value)
    if value and check_file:
        assert os.path.exists(value), "File not found: %s" % value
    return value
    

#def make_filename(sample_filename, *args):
#    # Create a new filename using information from a sample.
#    # <PATH>/<ROOT>.<EXT>


def calc_max_procs_from_ram(gb_per_proc, buffer=8):
    # Given the number of gigabytes each processor will need,
    # calculate the maximum number of processes that should be run
    # concurrently.
    # buffer is the number of gb to leave for other processes.
    total_bytes = get_physical_memory()
    total_gb = total_bytes/(1024*1024*1024)
    max_procs = (total_gb-buffer)/gb_per_proc
    max_procs = max(max_procs, 1)
    return max_procs


def get_physical_memory():
    # Return the amount of RAM in this machine in bytes.
    import os
    page_size = os.sysconf("SC_PAGE_SIZE")
    num_pages = os.sysconf("SC_PHYS_PAGES")
    return page_size * num_pages

    
def get_config(name):
    from genomicode import filelib
    from genomicode import config
    return filelib.which_assert(getattr(config, name))
