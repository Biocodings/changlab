"""
Objects:
IdentifiedDataNode

Functions:

# Network and Nodes
get_inputid
high_light_path     Does something to network.

# Network
download_ftp
download_dataset   # move to GEO?

# Zip
gunzip
extract_from_zip
unzip_if_zip

# File operations
which             Find full path of executable program.
exists_nz
merge_two_files

# Matrix operations
is_missing        If a matrix has missing values.  Move?
format_convert    Transpose matrix?
replace_matrix_header
convert_gene_list_platform
convert_to_same_platform

# Outputs
write_Betsy_report_parameters_file
plot_line_keywds
plot_line_keywd
find_pcaplots

# Miscellaneous
is_number
renew_parameters    Delete a list of keys from a dictionary.

find_fastq_files         Find all fastq files in a folder.
find_merged_fastq_files  Find the fastq generated by merge_reads.
safe_mkdir
find_fasta_files         Find all fasta files in a folder.

find_picard_jar

run_parallel
run_single

read_sample_group_file
fix_sample_group_filenames
assert_sample_group_file

shellquote

assert_bin

find_bowtie1_reference
make_bowtie1_command
parse_bowtie1_output

find_bowtie2_reference
make_bowtie2_command
parse_bowtie2_output

find_bwa_reference

"""
# _find_ids_that_pass_filters


#FMT = "%a %b %d %H:%M:%S %Y"

# DataNode + identifier.
class IdentifiedDataNode:
    def __init__(self, data, identifier=""):
        self.data = data
        self.identifier = identifier
    def __repr__(self):
        x = str(self.data) + ' identifier:' + self.identifier
        return x


## class AntecedentFilter:
##     def __init__(self, datatype_name=None, contents=None, **attributes):
##         self.datatype_name = datatype_name
##         self.contents = contents
##         self.attributes = attributes
##         self.mismatch_reason = None
##     def matches_node(self, node):
##         if self.datatype_name and self.datatype_name != node.datatype.name:
##             self.mismatch_reason = "datatype_name"
##             return False
##         if self.contents and node.attributes.get("contents") != self.contents:
##             self.mismatch_reason = "contents"
##             return False
##         for (key, value) in self.attributes.iteritems():
##             if node.attributes.get(key) != value:
##                 self.mismatch_reason = "attributes: %s" % key
##                 return False
##         self.mismatch_reason = None
##         return True
##     def __str__(self):
##         return self.__repr__()
##     def __repr__(self):
##         x = [self.datatype_name, repr(self.contents)]
##         for name, value in self.attributes.iteritems():
##             # Bug: strings should be hashed.
##             x.append("%s=%s" % (name, value))
##         x = "%s(%s)" % (self.__class__.__name__, ", ".join(x))
##         return x


## def _find_ids_that_pass_filters(network, node_ids, filters):
##     # For each of the filters, pick out one of the nodes.  Return a
##     # list of nodes parallel to filters, or None if not found

##     matches = []
##     for f in filters:
##         for id_ in node_ids:
##             if id_ in matches:  # don't reuse nodes
##                 continue
##             if f.matches_node(network.nodes[id_]):
##                 matches.append(id_)
##                 break
##         else:
##             return None
##     assert len(matches) == len(filters)
##     return matches


## def find_antecedents(network, module_id, user_attributes, pool, *filters):
##     # filters should be AntecedentFilter objects.  Return either a
##     # IdentifiedDataNode (if 0 or 1 filters), or a list of
##     # IdentifiedDataNodes parallel to filters.  Raises an exception if
##     # no antecedents could be found.
##     import os
##     import bie3

##     module_name = network.nodes[module_id].name

##     # Make a list of every possible combination of inputs that goes
##     # into this module.
##     prev_ids = []
##     for id_ in network.transitions:
##         if module_id in network.transitions[id_]:
##             prev_ids.append(id_)
##     all_input_ids = bie3._get_valid_input_combinations(
##         network, module_id, prev_ids, user_attributes)

##     # Filter for just the combinations in which all input nodes have
##     # been run.
##     filtered = []
##     for input_ids in all_input_ids:
##         # If not all the input nodes have been run, then ignore this
##         # combination.
##         x = [x for x in input_ids if x in pool]
##         if len(x) != len(input_ids):
##             continue
##         filtered.append(x)
##     all_input_ids = filtered

##     # Filter based on the user criteria.
##     ids = None
##     if not filters:
##         # If no filters given, then just return the first id found.
##         assert all_input_ids
##         assert all_input_ids[0]
##         ids = [all_input_ids[0][0]]
##     else:
##         for input_ids in all_input_ids:
##             ids = _find_ids_that_pass_filters(network, input_ids, filters)
##             if ids is not None:
##                 break
##     assert ids, "antecedents not found: %s" % module_name
##     for id_ in ids:
##         if not pool[id_].identifier:
##             continue
##         assert os.path.exists(pool[id_].identifier), (
##             "File not found: %s" % pool[id_].identifier)
##     objs = [pool[x] for x in ids]
##     assert not filters or len(objs) == len(filters)
##     # Return a single IdentifiedDataNode if 0 or 1 filters given.
##     if len(objs) <= 1:
##         objs = objs[0]
##     return objs


## def get_identifier(
##     network, module_id, pool, user_attributes, datatype=None, contents=None,
##     optional_key=None, optional_value=None, second_key=None, second_value=None,
##     **param):
##     # Returns a single IdentifiedDataNode that goes into this module.  What is
##     # this used for?
##     import os
##     import bie3

##     assert not (optional_key and not optional_value)
##     assert not (optional_value and not optional_key)
##     assert not (second_key and not second_value)
##     assert not (second_value and not second_key)

##     # Make a list of every possible combination of inputs that goes
##     # into this module.
##     prev_ids = []
##     for id_ in network.transitions:
##         if module_id in network.transitions[id_]:
##             prev_ids.append(id_)
##     all_input_ids = bie3._get_valid_input_combinations(
##         network, module_id, prev_ids, user_attributes)

##     # Filter for just the all_input_ids in which all input nodes have
##     # been run.
##     filtered = []
##     for input_ids in all_input_ids:
##         # If not all the input nodes have been run, then ignore this
##         # combination.
##         x = [x for x in input_ids if x in pool]
##         if len(x) != len(input_ids):
##             continue
##         filtered.append(x)
##     all_input_ids = filtered

##     # Filter based on the user criteria.
##     ids = []
##     for input_ids in all_input_ids:
##         for id_ in input_ids:
##             if id_ in ids:
##                 continue
##             node = network.nodes[id_]
##             if datatype and node.datatype.name != datatype:
##                 continue
##             if contents and node.attributes.get("contents") != contents:
##                 continue
##             if optional_key and \
##                    node.attributes.get(optional_key) != optional_value:
##                 continue
##             if second_key and \
##                    node.attributes.get(second_key) != second_value:
##                 continue
##             all_found = True
##             for key in param:
##                 if node.attributes.get(key) != param[key]:
##                     all_found = False
##             if not all_found:
##                 continue
##             ids.append(id_)

##     assert ids, 'cannot find node that match for %s' % \
##                network.nodes[module_id].name)
##     for id_ in ids:
##         if pool[i].identifier:
##             assert os.path.exists(pool[i].identifier), (
##                 'the input file %s for %s does not exist' %
##                 (pool[i].identifier, network.nodes[module_id].name))
##     id_ = ids[0]
##     return pool[id_]


# Not sure exactly what this does.  Looks like it takes some sort of
# file identifier, and returns some sort of cleaned up id.
def get_inputid(identifier):
    import os

    # identifier is in format:
    # <data type>_<file>.<ext>
    # class_label_<user_given_name>.cls
    # Pull out just the <user_given_name>.
    x = identifier
    x = os.path.split(x)[-1]     # get file (no path)
    x = os.path.splitext(x)[-2]  # get base name (no extension)
    x = x.split("_")[-1]         # ???
    return x
    #old_filename = os.path.split(identifier)[-1]
    #old_filename_no_ext = os.path.splitext(old_filename)[-2]
    #inputid = old_filename_no_ext.split('_')[-1]
    #return inputid


# Why is this here?  And why is rma hard coded?
def find_pcaplots(network, pool, module_id, rma=False):
    import os

    before_pcaplot = None
    after_pcaplot = None
    for x in pool:
        node, node_id = pool[x], x
        if not node.data.datatype.name == 'PcaPlot':
            continue
        if module_id in network.transitions[node_id]:
            assert os.path.exists(node.identifier), (
                'the input file %s for %s does not exist' %
                (node.identifier, network.nodes[module_id].name))
            if (node.data.attributes['quantile_norm'] == 'no' and
                node.data.attributes['combat_norm'] == 'no' and
                node.data.attributes['shiftscale_norm'] == 'no' and
                node.data.attributes['bfrm_norm'] == 'no' and
                node.data.attributes['dwd_norm'] == 'no' and
                node.data.attributes['gene_center'] == 'no' and
                node.data.attributes['gene_normalize'] == 'no' and
                node.data.attributes['unique_genes'] == 'no' and
                node.data.attributes['platform'] == 'no' and
                node.data.attributes['duplicate_probe'] == 'no' and
                node.data.attributes['group_fc'] == 'no'):
                before_pcaplot = node
            else:
                after_pcaplot = node
        if not after_pcaplot:
            after_pcaplot = before_pcaplot
    return before_pcaplot, after_pcaplot


# Why is this here?  Should also be named has_missing_values?
def is_missing(identifier):
    import arrayio

    M = arrayio.read(identifier)
    has_missing = False
    for i in range(M.dim()[0]):
        for j in range(M.dim()[1]):
            if M._X[i][j] is None:
                has_missing = True
                break
        if has_missing:
            break
    return has_missing


def merge_two_files(A_file, B_file, handle):
    """input two files and merge, write the output to handle"""
    import arrayio
    from genomicode import Matrix
    from genomicode import matrixlib

    M_A = arrayio.read(A_file)
    M_B = arrayio.read(B_file)
    assert arrayio.tab_delimited_format.is_matrix(M_A)
    assert arrayio.tab_delimited_format.is_matrix(M_B)
    [M_A, M_B] = matrixlib.align_rows(M_A, M_B)
    assert M_A.nrow() > 0, 'there is no common genes between two files'
    X = []
    for i in range(M_A.dim()[0]):
        x = M_A._X[i] + M_B._X[i]
        X.append(x)
    row_names = M_A._row_names
    row_order = M_A._row_order
    col_names = {}
    for name in M_A._col_names:
        if name not in M_B._col_names:
            continue
        newsample_list = []
        for sample in M_B._col_names[name]:
            if sample in M_A._col_names[name]:
                newsample = sample + '_2'
            else:
                newsample = sample
            newsample_list.append(newsample)
        #x = M_A._col_names[name] + M_B._col_names[name]
        x = M_A._col_names[name] + newsample_list
        col_names[name] = x
    M_c = Matrix.InMemoryMatrix(X, row_names, col_names, row_order)
    arrayio.tab_delimited_format.write(M_c, handle)


def which(program):
    import os

    is_jar = program.lower().endswith(".jar")

    def is_exe(fpath):
        return os.path.exists(fpath) and (os.access(fpath, os.X_OK) or is_jar)

    def ext_candidates(fpath):
        yield fpath
        for ext in os.environ.get("PATHEXT", "").split(os.pathsep):
            yield fpath + ext

    fpath, fname = os.path.split(program)
    if fpath:
        if is_exe(program):
            return program
    else:
        for path in os.environ["PATH"].split(os.pathsep):
            exe_file = os.path.join(path, program)
            for candidate in ext_candidates(exe_file):
                if is_exe(candidate):
                    return candidate
    return None


def which_assert(binary):
    # Make sure a binary exists and return its realpath.
    which_binary = which(binary)
    assert which_binary, "Cannot find: %s" % binary
    return which_binary
              

def format_convert(X):
    data = []
    for i in range(X.dim()[1]):
        data.append(X.value(None, i))
    return data


def write_Betsy_report_parameters_file(
    inputs, outfile, starttime, user, job_name):
    import os
    import json
    import time
    #from stat import *

    st = os.stat(outfile)
    modified_time = time.asctime(time.localtime(st[time.ST_MTIME]))
    f = file(os.path.join(os.getcwd(), 'Betsy_parameters.txt'), 'w')
    if isinstance(inputs, list):
        text = ['Module input:', [
            ('', i) for i in inputs
        ], 'Module output:', os.path.split(outfile)[-1],
                'Module output parameters:', '', 'Pipeline module sequence:',
                '', 'Start time:', starttime, 'Finish time:', modified_time,
                'User:', user, 'Jobname:', job_name]
    else:
        text = ['Module input:', ('', inputs), 'Module output:',
                os.path.split(outfile)[-1], 'Module output parameters:', '',
                'Pipeline module sequence:', '', 'Start time:', starttime,
                'Finish time:', modified_time, 'User:', user, 'Jobname:',
                job_name]
    newtext = json.dumps(text, sort_keys=True, indent=4)
    f.write(newtext)
    f.close()


def exists_nz(filename):
    """check if filename (file or path) exists and not empty"""
    import os

    if not os.path.exists(filename):  # does not exist
        return False
    if os.path.isdir(filename):  # is directory and not empty
        if os.listdir(filename):
            return True
        return False
    size = os.path.getsize(filename)  #is file and not empty
    if size > 0:
        return True
    return False


def plot_line_keywds(filename, keywords, outfile):
    import arrayio
    from genomicode import mplgraph

    M = arrayio.read(filename)
    header = M.row_names()
    label = M._col_names['_SAMPLE_NAME']
    outfiles = []
    for keyword in keywords:
        out = keyword + '.png'
        lines = []
        data = []
        legend_name = []
        for i in range(M.dim()[0]):
            if M.row_names(header[1])[i] == keyword:
                data.append(M.slice()[i])
                legend_name.append(M.row_names(header[0])[i])
        assert len(data) > 0, 'cannot find the keywords %s in the file %s' % (
            keywords, filename)
        for i in range(len(data)):
            line = [(j, data[i][j]) for j in range(len(data[i]))]
            lines.append(line)
        params = {
            "box_label" : label,
            "legend" : legend_name,
            "ylim_min" : 0,
            "ylabel" : keyword,
            "left" : 0.1,
            }
        fig = mplgraph.lineplot(*lines, **params)
        fig.savefig(out)
        outfiles.append(out)
    import Image
    img_w_list = []
    img_h_list = []
    imgs = []
    for i in range(len(outfiles)):
        img = Image.open(outfiles[i], 'r')
        img_w, img_h = img.size
        img_w_list.append(img_w)
        img_h_list.append(img_h)
        imgs.append(img)
    total_w = max(img_w_list) + 30
    total_h = sum(img_h_list) + 10
    background = Image.new('RGBA', (total_w, total_h), (255, 255, 255, 255))
    bg_w, bg_h = background.size
    offset_w = (bg_w - max(img_w_list)) / 2
    offset_h_list = []
    for i in range(len(img_h_list)):
        offset_h = bg_h - sum(img_h_list[i:])
        offset_h_list.append(offset_h)
    for img, offset_h in zip(imgs, offset_h_list):
        background.paste(img, (offset_w, offset_h))
    background.save(outfile)
    assert exists_nz(outfile), 'the plot_line_keywds fails'


def plot_line_keywd(filename, keyword, outfile):
    import arrayio
    from genomicode import mplgraph

    M = arrayio.read(filename)
    header = M.row_names()
    label = M._col_names['_SAMPLE_NAME']
    lines = []
    data = []
    legend_name = []
    for i in range(M.dim()[0]):
        if M.row_names(header[1])[i] == keyword:
            data.append(M.slice()[i])
            x = "%s (%s)" % (keyword, M.row_names(header[0])[i])
            legend_name.append(x)
    assert len(data) > 0, 'cannot find the keyword %s in the file %s' % (
        keyword, filename)
    for i in range(len(data)):
        line = [(j, data[i][j]) for j in range(len(data[i]))]
        lines.append(line)
    params = {
        "box_label" : label,
        "legend" : legend_name,
        "ylim_min" : 0,
        "ylabel" : "Signal",
        "left" : 0.1,
        }
    fig = mplgraph.lineplot(*lines, **params)
    fig.savefig(outfile)
    assert exists_nz(outfile), 'the plot_line_keywd fails'


def renew_parameters(parameters, key_list):
    newparameters = parameters.copy()
    for key in key_list:
        if key in newparameters.keys():
            del newparameters[key]
    return newparameters


def is_number(s):
    try:
        float(s)
    except ValueError:
        return False
    return True


def download_ftp(host, path, filename):
    import ftplib
    from ftplib import FTP
    import socket
    try:
        ftp = FTP(host)
    except (socket.error, socket.gaierror), e:
        raise AssertionError('Error [%s]: cannot reach %s' % (str(e), host))
    try:
        ftp.login()
    except ftplib.error_perm, e:
        ftp.quit()
        raise AssertionError('Error [%s] :cannot login anonymously' % str(e))
    try:
        ftp.cwd(path)
    except ftplib.error_perm, x:
        if str(x).find('No such file') >= 0:
            raise AssertionError('cannot find the %s' % path)
    filelist = []  # to store all files
    ftp.retrlines('NLST', filelist.append)
    if filename in filelist:
        f = open(filename, 'wb')
        ftp.retrbinary('RETR ' + filename, f.write)
        f.close()
        ftp.close()
    else:
        ftp.close()
        raise AssertionError('cannot find %s in %s' % (filename, host))


def download_dataset(GSEID):
    import os
    import tarfile

    #download the tar folder from geo
    host = 'ftp.ncbi.nih.gov'
    GSE_directory = 'pub/geo/DATA/supplementary/series/' + GSEID
    download_rarfile = GSEID + '_RAW.tar'
    download_ftp(host, GSE_directory, download_rarfile)
    #untar the data folder
    GSEID_path = GSEID
    if not tarfile.is_tarfile(download_rarfile):
        raise ValueError('download file is not tar file')
    tar = tarfile.open(download_rarfile)
    tar.extractall(path=GSEID_path)
    tar.close()
    os.remove(download_rarfile)
    assert os.path.exists(GSEID_path), 'the download file %s\
                        does not exist' % GSEID_path
    assert len(os.listdir(GSEID_path)) > 0, 'the untar in \
           download_geo_dataset_GPL %s fails' % GSEID_path
    return GSEID_path


def gunzip(filename):
    import os
    import gzip
    import userfile

    x = userfile._unhash_storefile(filename)
    real_name = x
    if isinstance(x, tuple):
        real_name = x[1]
    if filename.endswith('.gz') or real_name.endswith('.gz'):
        newfilename = os.path.join(
            os.getcwd(), os.path.split(os.path.splitext(filename)[0])[-1])
        #unzip the gz data
        fileObj = gzip.GzipFile(filename, 'rb')
        fileObjOut = file(newfilename, 'wb')
        while 1:
            line = fileObj.readline()
            if line == '':
                break
            fileObjOut.write(line)
        fileObj.close()
        fileObjOut.close()
        assert os.path.exists(newfilename), (
            'unzip the file %s fails' % filename
        )
        return newfilename
    else:
        return None


def high_light_path(network_file, pipeline, out_file):
    from xml.dom.minidom import parseString

    pipeline1 = ['start']
    pipeline1.extend(pipeline)
    f = open(network_file, 'r')
    data = f.read()
    f.close()
    dom = parseString(data)
    nodes = dom.getElementsByTagName('node')
    edges = dom.getElementsByTagName('edge')
    for analysis in pipeline1:
        for node in nodes:
            value = node.childNodes[1].getAttributeNode('value').nodeValue
            if value == analysis:
                node.childNodes[11].attributes['fill'] = '#FF33CC'
    for i in range(len(pipeline1[:-1])):
        label = pipeline1[i] + ' (pp) ' + pipeline1[i + 1]
        for edge in edges:
            if edge.childNodes[1].getAttributeNode('value').nodeValue == label:
                edge.childNodes[7].attributes['width'] = '6'
    xmlstr = dom.toxml('utf-8')
    f = open(out_file, 'w')
    f.write(xmlstr)
    f.close()


def convert_gene_list_platform(genes, platform):
    from genomicode import jmath
    from genomicode import arrayplatformlib

    platform_list = [i.name for i in arrayplatformlib.platforms]
    assert platform in platform_list, (
        'we cannot convert to the platform %s' % platform
    )
    chip = arrayplatformlib.guess_chip_from_probesets(genes)
    assert chip, 'we cannot guess the platform for the input file'
    in_attribute = arrayplatformlib.get_bm_attribute(chip)
    in_mart = arrayplatformlib.get_bm_organism(chip)
    out_attribute = arrayplatformlib.get_bm_attribute(platform)
    out_mart = arrayplatformlib.get_bm_organism(platform)
    R = jmath.start_R()
    jmath.R_equals_vector(genes, 'gene_id')
    R('library(biomaRt)')
    jmath.R_equals(in_attribute, 'in_attribute')
    jmath.R_equals(in_attribute, 'filters')
    jmath.R_equals(in_mart, 'in_mart')
    R('old=useMart("ensembl",in_mart)')
    jmath.R_equals(out_attribute, 'out_attribute')
    jmath.R_equals(out_mart, 'out_mart')
    R('new=useMart("ensembl",out_mart)')
    R(str('homolog = getLDS(attributes=in_attribute,') +
      str('filters=filters,values=gene_id,mart=old,') +
      str('attributesL=out_attribute,martL=new)'))
    homolog = R['homolog']
    #old_id = [str(i) for i in homolog[0]]
    human_id = [str(i) for i in homolog[1]]
    return human_id


def convert_to_same_platform(filename1, filename2, platform=None):
    import arrayio
    import subprocess
    from genomicode import config
    from genomicode import arrayplatformlib

    M1 = arrayio.read(filename1)
    platform1 = arrayplatformlib.identify_platform_of_matrix(M1)
    M2 = arrayio.read(filename2)
    platform2 = arrayplatformlib.identify_platform_of_matrix(M2)
    if platform1 == platform2:
        return filename1, filename2

    Annot_path = config.annotate_matrix
    Annot_BIN = which(Annot_path)
    assert Annot_BIN, 'cannot find the %s' % Annot_path
    if platform1 == platform:
        filename = filename2
        newfilename1 = filename1
        newfilename2 = 'tmp'
    elif platform2 == platform:
        filename = filename1
        newfilename1 = 'tmp'
        newfilename2 = filename2
        
    if platform:
        command = [
            'python', Annot_BIN, '-f', filename, '-o', 'tmp',
            "--platform", platform]
        process = subprocess.Popen(
            command, shell=False, stdout=subprocess.PIPE,
            stderr=subprocess.PIPE)
        error_message = process.communicate()[1]
        if error_message:
            raise ValueError(error_message)
        #assert module_utils.exists_nz('tmp'), (
        #    'the platform conversion fails')
        assert exists_nz('tmp'), 'the platform conversion fails'
    return newfilename1, newfilename2


    ##def extract_from_zip(zipName):
    ##    z = zipfile.ZipFile(zipName)
    ##    for f in z.namelist():
    ##        if f.endswith('/'):
    ##            os.makedirs(f)
    ##        else:
    ##            z.extract(f)


def extract_from_zip(zipName, outdir):
    import zipfile

    x = zipfile.ZipFile(zipName)
    x.extractall(path=outdir)


def unzip_if_zip(input_name):
    import os
    import zipfile

    if zipfile.is_zipfile(input_name):
        directory = os.path.split(input_name)[-1]
        directory = os.path.splitext(directory)[0]
        directory = os.path.join(os.getcwd(), directory)
        extract_from_zip(input_name, directory)
        for dirname in os.listdir(directory):
            if not dirname == '__MACOSX':
                directory = os.path.join(directory, dirname)
    else:
        directory = input_name
    return directory


## def replace_matrix_header(M, old_header, new_header):
##     # Actually converts from GCT to TDF format.
##     M = M.matrix()
##     assert old_header in M._row_order
##     M._row_names[new_header] = M._row_names[old_header]
##     del M._row_names[old_header]
##     ids = M._row_order
##     ids = [
##         x.replace(old_header, new_header) if x == old_header else x
##         for x in ids]
##     M._row_order = ids
##     return M


## def process_group_info(group_file):
##     """return a dict with <sample_name:[[left_sample_list],
##                                         [right_sample_list]]"""
##     f = file(group_file, 'r')
##     text = f.readlines()
##     f.close()
##     group_dict = {}
##     text = [line.strip() for line in text if line.strip()]
##     for line in text:
##         words = line.split('\t')
##         if len(words) == 3:
##             if words[0] not in group_dict:
##                 group_dict[words[0]] = [words[2]]
##             else:
##                 group_dict[words[0]].append(words[2])
##         elif len(words) == 4:
##             if words[0] not in group_dict:
##                 group_dict[words[0]] = [[words[2]], [words[3]]]
##             else:
##                 group_dict[words[0]][0].append(words[2])
##                 group_dict[words[0]][1].append(words[3])
##         else:
##             raise ValueError('group file is invalid')
##     return group_dict

##def concatenate_files(input_files,outfile):
##    with open(outfile,'w') as outfile:
##        for fname in input_files:
##            with gzip.open(fname) as infile:
##                for line in infile:
##                    outfile.write(line)
##                    
##def concatenate_multiple_line(group_dict,foldername):
##    current_dir = os.getcwd()
##    new_group_dict = {}
##    for sample_name, files in group_dict.iteritems():
##        if len(files)==1:
##            outfile = os.path.join(current_dir,sample_name+'.fastq')
##            inputfiles = [os.path.join(foldername,x) for x in files[0]]
##            concatenate_files(inputfiles,outfile)
##            new_group_dict[sample_name]=[outfile]
##        elif len(files)==2:
##            outfile_left = os.path.join(current_dir,sample_name+'_R1.fastq')
##            outfile_right = os.path.join(current_dir,sample_name+'_R2.fastq')
##            inputfiles_left = [os.path.join(foldername,x) for x in files[0]]
##            inputfiles_right = [os.path.join(foldername,x) for x in files[1]]
##    	    concatenate_files(inputfiles_left,outfile_left)
##            concatenate_files(inputfiles_right,outfile_right)
##            new_group_dict[sample_name] = [outfile_left,outfile_right]
##    return new_group_dict
##   
##           
##a=process_group_info('/home/xchen/NGS/try_RSEM/big_sample_data/sample_group.txt')
##b=concatenate_multiple_line(a,'/home/xchen/NGS/try_RSEM/big_sample_data/')
##print b


def find_fastq_files(path):
    # Return a list of the FASTQ files (full filenames) under path.
    import os
    from genomicode import filelib
    
    filenames = filelib.list_files_in_path(path)
    # Filter out the fastq files.
    i = 0
    while i < len(filenames):
        p, f = os.path.split(filenames[i])
        f_l = f.lower()
        if f.startswith("."):
            del filenames[i]
        elif f_l.find(".fq") >= 0:
            i += 1
        elif f_l.find(".fastq") >= 0:
            i += 1
        else:
            del filenames[i]
    return filenames


def find_fasta_files(path):
    # Return a list of the FASTA files (full filenames) under path.
    import os
    from genomicode import filelib

    fa_ext = [".fa", ".fasta"]
    compress_ext = ["", ".gz", ".xz", ".bz2"]
    EXTENSIONS = []
    for x1 in fa_ext:
        for x2 in compress_ext:
            x = "%s%s" % (x1, x2)
            EXTENSIONS.append(x)
    
    filenames = filelib.list_files_in_path(path)
    # Filter out the fastq files.
    i = 0
    while i < len(filenames):
        p, f = os.path.split(filenames[i])
        f_l = f.lower()
        if f.startswith("."):
            del filenames[i]
            continue
        for x in EXTENSIONS:
            if f_l.endswith(x):
                i += 1  # is fasta file
                break
        else:
            del filenames[i]
    return filenames


def find_bam_files(path):
    # Return a list of the .bam files (full filenames) under path.
    import os
    from genomicode import filelib
    
    filenames = filelib.list_files_in_path(path)
    # Filter out the bam files.
    i = 0
    while i < len(filenames):
        p, f = os.path.split(filenames[i])
        f_l = f.lower()
        if f.startswith("."):
            del filenames[i]
        elif f_l.endswith(".bam"):
            i += 1
        else:
            del filenames[i]
    return filenames


def find_merged_fastq_files(sample_group_filename, fastq_path):
    # Read the sample group file.  Return a list of (sample,
    # pair1.fastq, pair2.fastq).  pair2.fastq will be None for single
    # end reads.  Both files are full paths.
    import os
    
    sample_groups = read_sample_group_file(sample_group_filename)
    x = [x[1] for x in sample_groups]
    x = sorted({}.fromkeys(x))
    all_samples = x

    # Find the fastq files for each of the samples.
    # The merge_reads module will save fastq files in the format:
    # <fastq_path>/<sample>.fastq          # if single end
    # <fastq_path>/<sample>_<Pair>.fastq   # if paired end

    fastq_files = []
    for sample in all_samples:
        # Look for single or paired end fastq files.
        pair1 = pair2 = None
        se_file = os.path.join(fastq_path, "%s.fastq" % sample)
        pe_file1 = os.path.join(fastq_path, "%s_1.fastq" % sample)
        pe_file2 = os.path.join(fastq_path, "%s_2.fastq" % sample)
        if os.path.exists(se_file):
            pair1 = se_file
            assert not os.path.exists(pe_file1)
            assert not os.path.exists(pe_file2)
        elif os.path.exists(pe_file1):
            assert not os.path.exists(se_file)
            assert os.path.exists(pe_file2)
            pair1 = pe_file1
            pair2 = pe_file2
        else:
            raise AssertionError, "Not found: %s" % sample

        x = sample, pair1, pair2
        fastq_files.append(x)

    # Make sure all samples are unique.
    x1 = [x[0] for x in fastq_files]
    x2 = {}.fromkeys(x1).keys()
    assert len(x1) == len(x2), "dup sample"
        
    return fastq_files


def safe_mkdir(path):
    import os
    
    if not os.path.exists(path):
        os.mkdir(path)


def run_parallel(commands, max_procs=None):
    # commands is a list of shell commands to run.  Return the output
    # as a single string.
    import subprocess
    
    #cat run01.sh | parallel -j <num_cores>
    cmd = [
        "parallel",
        ]
    if max_procs is not None:
        assert max_procs >= 0 and max_procs < 100
        cmd.extend(["-j", str(max_procs)])

    p = subprocess.Popen(
        cmd, bufsize=0, stdin=subprocess.PIPE,
        stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True)
    x = "\n".join(commands)
    x = p.communicate(x)
    data_out, data_err = x

    x = "%s\n%s" % (data_out, data_err)
    x = x.strip()
    return x


def run_single(command):
    # command is a string or list (see subprocess) for a single
    # command.  Return the output as a single string.
    import subprocess
    if type(command) != type(""):
        command = " ".join(command)
    x = subprocess.check_output(command, stderr=subprocess.STDOUT, shell=True)
    return x


def read_sample_group_file(file_or_handle):
    # Return list of (filename, sample, pair).  pair is None, 1, or 2.
    # 
    # Reads can be split across multiple files (e.g. for multiple
    # lanes), or across pairs.
    # Headers:
    # Filename  Sample  Pair
    # F1         A       1
    # F3         A       2
    # F2         A       1
    # F4         A       2
    # F5         B       1
    # F6         B       2
    #
    # - Filenames should be unique.
    # - Pair should be 1 or 2.  If single end reads, just leave blank.
    # - There can be many Filenames per Sample.  There can be many
    #   Pairs per Sample (if the reads for one pair are split).
    # - The pairs that match (1 to its 2 partner) should be next to
    #   each other in the file.
    import os
    from genomicode import filelib
    
    handle = file_or_handle
    if type(handle) is type(""):
        assert os.path.exists(file_or_handle)
        handle = filelib.openfh(handle)
        
    data = []
    for d in filelib.read_row(handle, header=1):
        pair = d.Pair.strip()
        assert pair in ["", "1", "2"], "Invalid pair: %s" % d.Pair
        x = d.Filename, d.Sample, pair
        data.append(x)
        
    # Make sure filenames are unique.
    seen = {}
    for x in data:
        filename, sample, pair = x
        assert filename not in seen, "Filenames not unique: %s" % filename
        seen[filename] = 1

    # For each sample, make sure there isn't a mix of paired and
    # single ended files.  It must be all single ended or all paired.
    x = [x[1] for x in data]
    all_samples = sorted({}.fromkeys(x))
    for sample in all_samples:
        x = [x[2] for x in data if x[1] == sample]
        x = sorted({}.fromkeys(x))
        if x == [""]:  # All single
            continue
        elif x == ["1", "2"]:  # All paired
            continue
        raise AssertionError, "Weird pairing: %s" % sample

    # Make sure each pair is next to each other.
    for sample in all_samples:
        pairs = [x[2] for x in data if x[1] == sample]
        # Should be all "", or a pattern of "1", "2".
        x = {}.fromkeys(pairs).keys()
        if x == [""]:  # all ""
            continue
        assert len(x) % 2 == 0, "Weird pairing: %s" % sample
        for i in range(0, len(x), 2):
            assert x[i] == "1", "Weird pairing: %s" % sample
            assert x[i+1] == "2", "Weird pairing: %s" % sample
        
    return data


def fix_sample_group_filenames(sample_groups, fastq_path):
    # Sometimes the file names can change, due to compression or
    # uncompression.  Try to adjust for this.
    import os

    x = os.listdir(fastq_path)
    fastq_files = {}.fromkeys(x)

    EXTENSIONS = [".gz", ".bz2", ".xz"]

    for i in range(len(sample_groups)):
        file_, sample, pair = sample_groups[i]

        # Try the file itself.
        to_try = [file_]
        # If the file ends with any of these extensions, then try the
        # uncompressed file.
        filestem = None
        for ext in EXTENSIONS:
            if file_.lower().endswith(ext):
                filestem = file_[:-len(ext)]
                to_try.append(filestem)
                break
        # Try the file with a different extension.
        if filestem:
            for ext in EXTENSIONS:
                x = filestem + ext
                to_try.append(x)
        # Try the file with these extensions.
        for ext in EXTENSIONS:
            x = file_ + ext
            to_try.append(x)
        for test_file in to_try:
            if test_file in fastq_files:
                break
        else:
            # File not found.
            continue
        
        x = test_file, sample, pair
        sample_groups[i] = x
    return sample_groups
    

def assert_sample_group_file(filename, fastq_path):
    import os
    
    x = read_sample_group_file(filename)
    x = fix_sample_group_filenames(x, fastq_path)
    sample_groups = x

    # Make sure each file can be found in the fastq folder.
    for x in sample_groups:
        file_, sample, pair = x
        filename = os.path.join(fastq_path, file_)
        assert os.path.exists(filename), "Missing FASTQ file: %s" % file_

    # Make sure there are no duplicate files.
    files = [x[0] for x in sample_groups]
    file2count = {}
    for x in files:
        file2count[x] = file2count.get(x, 0) + 1
    dups = sorted([x for x in file2count if file2count[x] > 1])
    assert not dups, "Duplicate files"
    # Make sure there are no FASTQ files without samples.
    # BROKEN. Does not account for compression, e.g. .gz.
    #all_filenames = [x[0] for x in data]
    #files = os.listdir(fastq_path)
    #for file in files:
    #    assert file in all_filenames, "Not in sample group file: %s" % file


def shellquote(s):
    return "'" + s.replace("'", "'\\''") + "'"

def find_picard_jar(jar_name):
    # jar_name should not include ".jar",
    # e.g. "AddOrReplaceReadGroups".
    import os
    from genomicode import config
    
    picard_path = config.picard
    assert os.path.exists(picard_path)
    jar_filename = os.path.join(picard_path, "%s.jar" % jar_name)
    assert os.path.exists(jar_filename)
    return jar_filename
    


def find_bowtie1_reference(search_path):
    # Find the indexed reference genome at:
    # <reference_path>/<assembly>.[1234].ebwt
    # <reference_path>/<assembly>.rev.[12].ebwt
    #
    # <reference_genome> is <reference_path>/<assembly>
    import os
    from genomicode import filelib
    
    x = filelib.list_files_in_path(search_path)
    x = [x for x in x if x.lower().endswith(".1.ebwt")]
    x = [x for x in x if not x.lower().endswith(".rev.1.ebwt")]
    assert len(x) == 1, "Cannot find bowtie index."
    x = x[0]
    reference_path, file_ = os.path.split(x)
    assembly = file_.replace(".1.ebwt", "")
    x = os.path.join(reference_path, assembly)
    return x

    
def find_bowtie2_reference(search_path):
    # Find the indexed reference genome at:
    # <reference_path>/<assembly>.[1234].bt2
    # <reference_path>/<assembly>.rev.[12].bt2
    #
    # <reference_genome> is <reference_path>/<assembly>
    import os
    from genomicode import filelib
    
    x = filelib.list_files_in_path(search_path)
    x = [x for x in x if x.lower().endswith(".1.bt2")]
    x = [x for x in x if not x.lower().endswith(".rev.1.bt2")]
    assert len(x) == 1, "Cannot find bowtie2 index."
    # Assume the rest of the files are present.
    x = x[0]
    reference_path, file_ = os.path.split(x)
    assembly = file_.replace(".1.bt2", "")
    x = os.path.join(reference_path, assembly)
    return x


def make_bowtie2_command(
    reference_genome, fastq_file1, fastq_file2=None, orientation=None,
    sam_file=None, num_threads=None):
    # Orientation must be None, "ff", "fr", "rf"
    import os
    from genomicode import config

    assert os.path.exists(fastq_file1)
    if fastq_file2:
        assert os.path.exists(fastq_file2)
    if orientation:
        assert orientation in ["ff", "fr", "rf"]
    if num_threads is not None:
        assert num_threads >= 1 and num_threads < 100
    
    
    bowtie2 = which_assert(config.bowtie2)

    # bowtie2 -p <nthreads> -x <reference_base> -U <sample.fq>
    #   -S <sample.sam>
    # bowtie2 -p <nthreads> -x <reference_base> -1 <sample_1.fq>
    #   -2 <sample_2.fq> --fr -S <sample.sam>

    sq = shellquote
    cmd = [
        sq(bowtie2),
        ]
    if num_threads:
        cmd += ["-p", str(num_threads)]
    cmd += ["-x", sq(reference_genome)]
    if not fastq_file2:
        cmd += [
            "-U", sq(fastq_file1),
            ]
    else:
        cmd += [
            "-1", sq(fastq_file1),
            "-2", sq(fastq_file2),
            ]
    if orientation:
        cmd += ["--%s" % orientation]
    if sam_file:
        cmd += [
            "-S", sq(sam_file),
            ]
    return " ".join(cmd)


def make_bowtie1_command(
    reference_genome, sam_file, fastq_file1, fastq_file2=None,
    orientation=None, num_threads=None):
    # Orientation must be None, "ff", "fr", "rf"
    import os
    from genomicode import config

    assert os.path.exists(fastq_file1)
    if fastq_file2:
        assert os.path.exists(fastq_file2)
    if orientation:
        assert orientation in ["ff", "fr", "rf"]
    if num_threads is not None:
        assert num_threads >= 1 and num_threads < 100
    
    bowtie1 = which_assert(config.bowtie)

    # bowtie --sam -p <nthreads> <reference_base> <sample.fq>
    #   <sample.sam>
    # bowtie --sam --fr -p <nthreads> <reference_base> -1 <sample_1.fq> -2
    #   <sample_2.fq> <sample.sam>

    sq = shellquote
    cmd = [
        sq(bowtie1),
        "--sam",
        ]
    if orientation:
        cmd += ["--%s" % orientation]
    if num_threads:
        cmd += ["-p", str(num_threads)]
    cmd += [sq(reference_genome)]
    if not fastq_file2:
        cmd += [sq(fastq_file1)]
    else:
        cmd += [
            "-1", sq(fastq_file1),
            "-2", sq(fastq_file2),
            ]
    cmd += [sq(sam_file)]
    return " ".join(cmd)


def parse_bowtie2_output(filename):
    # Return a dictionary with keys:
    # reads_processed    BOTH
    # aligned_reads      BOTH    (aligned >= 1 time)
    # concordant_reads   paired  (concordant >= 1 time)
    #
    # 1000 reads_processed means there are 1000 possible pairs.

    # For single-end reads.
    # 20000 reads; of these:
    #   20000 (100.00%) were unpaired; of these:
    #      1247 (6.24%) aligned 0 times
    #      18739 (93.69%) aligned exactly 1 time
    #      14 (0.07%) aligned >1 times
    # 93.77% overall alignment rate

    # For paired-end reads.
    # 62830141 reads; of these:
    #   62830141 (100.00%) were paired; of these:
    #     6822241 (10.86%) aligned concordantly 0 times
    #     48713485 (77.53%) aligned concordantly exactly 1 time
    #     7294415 (11.61%) aligned concordantly >1 times
    #     ----
    #     6822241 pairs aligned concordantly 0 times; of these:
    #       3004782 (44.04%) aligned discordantly 1 time
    #     ----
    #     3817459 pairs aligned 0 times concordantly or discordantly; of these:
    #       7634918 mates make up the pairs; of these:
    #         3135692 (41.07%) aligned 0 times
    #         1870375 (24.50%) aligned exactly 1 time
    #         2628851 (34.43%) aligned >1 times
    # 97.50% overall alignment rate
    from genomicode import filelib

    concordant_1 = concordant_more = None
    results = {}
    for line in filelib.openfh(filename):
        if line.find("reads; of these:") >= 0:
            x = line.strip().split()
            results["reads_processed"] = int(x[0])
        elif line.find("overall alignment rate") >= 0:
            x = line.strip().split()
            # 93.77% overall alignment rate
            x = x[0]            # 93.77%
            assert x.endswith("%")
            x = x[:-1]          # 93.77
            x = float(x) / 100  # 0.9377
            assert "reads_processed" in results
            aligned = int(x * results["reads_processed"])
            results["aligned_reads"] = aligned
        elif line.find("aligned concordantly exactly 1 time") >= 0:
            x = line.strip().split()
            x = x[0]
            concordant_1 = int(x)
        elif line.find("aligned concordantly >1 times") >= 0:
            x = line.strip().split()
            x = x[0]
            concordant_more = int(x)
    assert "reads_processed" in results
    assert "aligned_reads" in results

    if concordant_1 is not None:
        assert concordant_more is not None
    if concordant_more is not None:
        assert concordant_1 is not None
    if concordant_1 is not None:
        results["concordant_reads"] = concordant_1 + concordant_more
    return results


def parse_bowtie1_output(filename):
    # Return a dictionary with keys:
    # reads_processed
    # aligned_reads
    # 
    # Warning: Exhausted best-first chunk memory ...; skipping read
    # # reads processed: 62830141
    # # reads with at least one reported alignment: 28539063 (45.42%)
    # # reads that failed to align: 34291078 (54.58%)
    # Reported 28539063 paired-end alignments to 1 output stream(s)
    from genomicode import filelib
    
    results = {}
    for line in filelib.openfh(filename):
        if line.startswith("Warning:"):
            continue
        elif line.startswith("# reads processed"):
            x = line.split(":")
            assert len(x) == 2
            results["reads_processed"] = int(x[1])
        elif line.startswith("# reads with at least one reported alignment"):
            x = line.split(":")
            assert len(x) == 2
            x = x[1].strip()
            x = x.split(" ")
            assert len(x) == 2
            results["aligned_reads"] = int(x[0])
        elif line.startswith("# reads that failed to align"):
            pass
        elif line.startswith("Reported "):
            pass
        else:
            raise AssertionError, "Unknown line: %s" % line.strip()
    return results


def find_bwa_reference(search_path):
    # Find the indexed reference genome at:
    # <path>/<assembly>.fa
    # Return "<path>/<assembly>.fa", which is used as input to the bwa
    # tools.
    #
    # Other files:
    # <path>/<assembly>.fa.amb .ann .bwt .pac .sa
    import os
    from genomicode import filelib
    
    x = os.listdir(search_path)
    x = [x for x in x if x.lower().endswith(".fa")]
    assert len(x) < 2, "Multiple fa files."
    assert len(x) == 1, "Cannot find bwa index."
    x = x[0]
    reference_fa = os.path.join(search_path, x)
    INDEX = [".amb", ".ann", ".bwt", ".pac", ".sa"]
    for x in INDEX:
        x = "%s%s" % (reference_fa, x)
        assert filelib.exists_nz(x), "Missing index file: %s" % x
    return reference_fa


def make_bwa_mem_command(
    reference_genome, sam_filename, err_filename, fastq_file1,
    fastq_file2=None, num_threads=None):
    import os
    from genomicode import config

    assert os.path.exists(reference_genome)
    assert os.path.exists(fastq_file1)
    if fastq_file2:
        assert os.path.exists(fastq_file2)
    if num_threads is not None:
        assert num_threads >= 1 and num_threads < 100
    
    bwa = which_assert(config.bwa)

    # bwa mem -t <num_cores> ref.fa read1.fq read2.fq > aln-pe.sam
    sq = shellquote
    cmd = [sq(bwa), "mem"]
    if num_threads:
        cmd += ["-t", str(num_threads)]
    cmd += [sq(reference_genome)]
    cmd += [sq(fastq_file1)]
    if fastq_file2:
        cmd += [sq(fastq_file2)]
    cmd += [
        "1>", sq(sam_filename),
        "2>", sq(err_filename),
        ]
    return " ".join(cmd)



def find_rsem_reference(search_path):
    # Find the indexed reference genome at:
    # <search_path> should be:
    #   /ref/
    # Where the actual index files are:
    #   /ref/hg19.transcripts.fa (etc.)
    #   <search_path>/<assembly>.transcripts.fa
    # Return <search_path>/.../<assembly>
    # 
    # This can be used as the <index_stem> parameter for
    # rsem-calculate-expression.
    from genomicode import filelib
    
    x = filelib.list_files_in_path(search_path)
    x = [x for x in x if x.lower().endswith(".idx.fa")]
    x = [x for x in x if not x.lower().endswith(".n2g.idx.fa")]
    assert x, "Cannot find rsem index."
    assert len(x) == 1, "Found multiple potential rsem indexes."
    x = x[0]
    assert x.endswith(".idx.fa")
    x = x[:-len(".idx.fa")]
    index_stem = x

    index_files = [
        "%s.chrlist" % index_stem,
        "%s.grp" % index_stem,
        "%s.idx.fa" % index_stem,
        "%s.n2g.idx.fa" % index_stem,
        "%s.seq" % index_stem,
        "%s.ti" % index_stem,
        "%s.transcripts.fa" % index_stem,
        ]
    for filename in index_files:
        assert exists_nz(filename), "Missing: %s" % filename
    return index_stem


def make_rsem_command(
    index_stem, sample_name, fastq_file1, fastq_file2=None, num_threads=None):
    # rsem-calculate-expression -p <num_cores> --output-genome-bam 
    #   --paired-end <file1.fastq> <file2.fastq>
    #   <index_stem> <sample_name> >& <sample_name>.log"
    # For strand-specific:
    #   --forward-prob 1.0
    #   --forward-prob 0.0
    # 
    # <index_stem> should be:
    #   /ref/hg19
    # Where the actual index files are:
    #   /ref/hg19.transcripts.fa (etc.)
    # <sample_name> is prefix for all output files,
    #   e.g. <sample_name>.genes.results
    # Orientation must be None, "ff", "fr", "rf"
    import os
    from genomicode import config

    assert os.path.exists(fastq_file1)
    if fastq_file2:
        assert os.path.exists(fastq_file2)
    if num_threads is not None:
        assert num_threads >= 1 and num_threads < 100
    
    rsem_calculate = which_assert(config.rsem_calculate)
    
    sq = shellquote
    cmd = [
        sq(rsem_calculate),
        ]
    if num_threads:
        cmd += ["-p", str(num_threads)]
    cmd += ["--output-genome-bam"]
    if not fastq_file2:
        cmd += [sq(fastq_file1)]
    else:
        cmd += [
            "--paired-end",
            sq(fastq_file1),
            sq(fastq_file2),
            ]
    cmd += [sq(index_stem), sq(sample_name)]
    return " ".join(cmd)


def find_rsem_result_files(search_path):
    # Find the files with rsem gene expression estimates.  Return list
    # of (sample, gene_filename, isoform_filename).  gene_filename or
    # isoform_filename can be None.
    import os
    from genomicode import filelib

    # Look for files in the format:
    # <sample>.genes.results
    # <sample>.isoforms.results
    x = filelib.list_files_in_path(search_path)
    x1 = [x for x in x if x.endswith(".genes.results")]
    x2 = [x for x in x if x.endswith(".isoforms.results")]
    sample2files = {}  # sample -> gene_filename, isoform_filename
    for gene_filename in x1:
        p, f = os.path.split(gene_filename)
        sample = f.replace(".genes.results", "")
        assert sample not in sample2files
        sample2files[sample] = gene_filename, None
    for isoform_filename in x2:
        p, f = os.path.split(gene_filename)
        sample = f.replace(".genes.results", "")
        gene_filename, x = sample2files.get(sample, (None, None))
        assert x is None
        sample2files[sample] = gene_filename, isoform_filename
    data = []  # list of (sample, gene_filename, isoform_filename)X
    for sample in sorted(sample2files):
        gene_filename, isoform_filename = sample2files[sample]
        x = sample, gene_filename, isoform_filename
        data.append(x)
    return data


def copytree_or_file_into_tree(in_file_or_path, out_path):
    # in_file_or_path can be a path or a file.  If it is a path,
    # copies:
    #   in_path_or_file -> out_path
    # If file:
    #   in_path_or_file -> out_path/file
    import os
    import shutil
    
    if not os.path.isdir(in_file_or_path):
        safe_mkdir(out_path)
        p, f = os.path.split(in_file_or_path)
        out_filename = os.path.join(out_path, f)
        shutil.copy2(in_file_or_path, out_filename)
    else:
        # Should delete if the out_path exists.
        assert not os.path.exists(out_path)
        shutil.copytree(in_file_or_path, out_path)
    
    
