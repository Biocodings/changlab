from Module import AbstractModule

class Module(AbstractModule):
    def __init__(self):
        AbstractModule.__init__(self)

    def run(
        self, network, antecedents, out_attributes, user_options, num_cores,
        out_path):
        import os
        from genomicode import filelib
        from genomicode import ngslib
        from genomicode import parallel
        from genomicode import alignlib
        from Betsy import module_utils as mlib

        bam_node, ref_node = antecedents
        bam_filenames = mlib.find_bam_files(bam_node.identifier)
        ref = alignlib.create_reference_genome(ref_node.identifier)
        filelib.safe_mkdir(out_path)
        metadata = {}

        features_bed = mlib.get_user_option(
            user_options, "features_bed", check_file=True)
        if features_bed:
            metadata["features_bed"] = features_bed

        # Applies to genomecov.
        min_coverage = user_options.get("ignore_coverage_below")
        if min_coverage == "":
            min_coverage = None
        if min_coverage is not None:
            min_coverage = int(min_coverage)
            assert min_coverage >= 0

        metadata["tool"] = "bedtools %s" % ngslib.get_bedtools_version()
        metadata["num_cores"] = num_cores
        metadata["commands"] = []

        # Set up the filenames.
        # list of (
        #   sample,
        #   orig_bam_filename,    Original bam filename.
        #   bam_filename,         bam file, after filtering out unmapped reads.
        #   genomecov_filename,   Generated by genomecov.  Histogram.
        #   histo_datafile,       Data file to generate histogram (from cov).
        #   histo_plotfile,       Histogram plot.
        #   histo_prismfile,      To make histogram in PRISM.
        #
        #   ONLY USED IF features_bed
        #   intervallist_file,    Made from BED file.
        #   cov_filename,         Generated by Picard.
        #   targetcov_filename,   Generated by Picard.  Per target coverage.
        #   log_filename,         Output from Picard.
        #   )
        opj = os.path.join
        jobs = []   # list of filelib.GenericObject
        for bam_filename in bam_filenames:
            # <in_path>/<sample>.bam
            in_path, sample, ext = mlib.splitpath(bam_filename)
            assert ext == ".bam"
            clean_bam_filename = opj(out_path, "%s.bam" % sample)
            assert clean_bam_filename != bam_filename
            genomecov_filename = opj(out_path, "%s.genomecov.txt" % sample)
            histo_datafile = opj(out_path, "%s.histo.txt" % sample)
            histo_plotfile = opj(out_path, "%s.histo.png" % sample)
            histo_prismfile = opj(out_path, "%s.prism.txt" % sample)

            intervallist_file = opj(out_path, "%s.interval.txt" % sample)
            cov_filename = opj(out_path, "%s.coverage.txt" % sample)
            targetcov_filename = opj(out_path, "%s.targetcov.txt" % sample)
            log_filename = opj(out_path, "%s.picard.log" % sample)

            x = filelib.GenericObject(
                sample=sample,
                orig_bam_filename=bam_filename,
                bam_filename=clean_bam_filename,
                genomecov_filename=genomecov_filename,
                histo_datafile=histo_datafile,
                histo_plotfile=histo_plotfile,
                histo_prismfile=histo_prismfile,
                intervallist_file=intervallist_file,
                cov_filename=cov_filename,
                targetcov_filename=targetcov_filename,
                log_filename=log_filename)
            #x = sample, bam_filename, genomecov_filename, \
            #    histo_datafile, histo_plotfile, histo_prismfile, \
            #    intervallist_file, cov_filename, targetcov_filename, \
            #    log_filename
            jobs.append(x)

        # Remove unmapped reads from the BAM files.
        # Need to remove the unmapped reads or Picard might complain:
        # Exception in thread "main"
        # htsjdk.samtools.SAMFormatException: SAM validation error:
        # ERROR: Record 154286082, Read name
        # DF9F08P1:326:C5KJFACXX:5:1304:12068:90850, MAPQ should be 0
        # for unmapped read.
        #
        # This can happen with BWA generated alignments.
        cmds = []
        for x in jobs:
            x = _make_samtools_filter_cmd(x.orig_bam_filename, x.bam_filename)
            cmds.append(x)
        parallel.pshell(cmds, max_procs=num_cores)
        x = [x.bam_filename for x in jobs]
        filelib.assert_exists_nz_many(x)
            
        # Generate the intervallist_file(s).
        if features_bed:
            cmds = []
            for x in jobs:
                args = x.intervallist_file, features_bed, x.bam_filename
                x = _make_intervallist_file, args, {}
                cmds.append(x)
            parallel.pyfun(cmds, num_procs=num_cores)

        # Make the commands to run picard.
        if features_bed:
            commands = []
            for x in jobs:
                x = _make_calculatehsmetrics_command(
                    x.intervallist_file, x.bam_filename, x.cov_filename,
                    x.targetcov_filename, ref.fasta_file_full, x.log_filename)
                commands.append(x)
            metadata["commands"].append(commands)
            parallel.pshell(commands, max_procs=num_cores)

            x1 = [x.cov_filename for x in jobs]
            x2 = [x.targetcov_filename for x in jobs]
            filelib.assert_exists_nz_many(x1+x2)

        # Use genomecov to count read depth.
        x = _run_genomecov(jobs, ref_node.identifier, num_cores)
        metadata["commands"].append(x)

        # Summarize the average read depth.
        summary_file = opj(out_path, "summary.xls")
        _summarize_average_read_depth(jobs, min_coverage, summary_file)

        # Make histograms of the distribution of the read depth for
        # each sample.
        for x in jobs:
            _make_histo_file(x.genomecov_filename, x.histo_datafile)

        # Delete the filtered BAM files to save space.
        for x in jobs:
            filelib.assert_exists_nz(x.bam_filename)
            os.unlink(x.bam_filename)
        return metadata


    def name_outfile(self, antecedents, user_options):
        return "coverage"


def _run_genomecov(jobs, reference_file, num_cores):
    from genomicode import parallel
    from genomicode import filelib
    from genomicode import ngslib

    # Set up the commands to run.
    commands = []
    for x in jobs:
        x = ngslib.make_bedtools_genomecov_command(
            x.bam_filename, reference_file, x.genomecov_filename)
        commands.append(x)
    parallel.pshell(commands, max_procs=num_cores)

    # Make sure the analysis completed successfully.
    x = [x.genomecov_filename for x in jobs]
    filelib.assert_exists_nz_many(x)

    return commands


def _summarize_average_read_depth(jobs, min_coverage, summary_file):
    import os
    import math
    from genomicode import parallel
    from genomicode import filelib
    from genomicode import ngslib

    # Read in the read depth.
    results = {}  # sample -> bedtools results
    for x in jobs:
        assert x.sample not in results
        results[x.sample] = ngslib.parse_bedtools_genomecov_results(
            x.genomecov_filename)

    # Calculate the average read depth for each sample.
    sample2depth = {}  # sample -> mean, sd of depth
    for sample in results:
        x = results[sample]
        x = [x for x in x if x[0] == "genome"]
        data = x
        assert data
        # Calculate the mean depth.
        sum_ = 0
        total_bases = 0
        for x in data:
            x, depth, num_bases, x, x = x
            if min_coverage is not None and depth < min_coverage:
                continue
            total_bases += num_bases
            sum_ = sum_ + depth * num_bases
        depth_mean = float(sum_) / total_bases
        # Calculate the SD depth.
        sumsq = 0
        for x in data:
            x, depth, num_bases, x, x = x
            if min_coverage is not None and depth < min_coverage:
                continue
            sumsq += ((depth-depth_mean)*num_bases)**2
        depth_sd = math.sqrt(sumsq / (total_bases-1))
        sample2depth[sample] = depth_mean, depth_sd

    # Read in the coverage calculated by picard, if present.
    mean_target_coverage = {}  # sample -> coverage
    for x in jobs:
        if not os.path.exists(x.cov_filename):
            continue
        cov_data = _read_coverage_file(x.cov_filename)
        assert "MEAN_TARGET_COVERAGE" in cov_data
        mean_target_coverage[x.sample] = float(
            cov_data["MEAN_TARGET_COVERAGE"])

    # Make a table of the results.
    mean = "Mean"
    if min_coverage is not None:
        mean = "Mean (>= %d reads)" % min_coverage
    table = []
    header = ["Sample", mean, "SD"]
    if mean_target_coverage:
        header.append("MEAN_TARGET_COVERAGE")
    table.append(header)
    for sample in sorted(sample2depth):
        mean, sd = sample2depth[sample]
        mean_s = "%.1f" % mean
        sd_s = "%.2f" % sd
        x = [sample, mean_s, sd_s]
        if mean_target_coverage:
            cov = mean_target_coverage[sample]
            x.append(cov)
        assert len(x) == len(header)
        table.append(x)

    # Write out the table as text file.
    TXT_FILE = "summary.txt"
    handle = open(TXT_FILE, 'w')
    for x in table:
        print >>handle, "\t".join(map(str, x))
    handle.close()

    sq = parallel.quote
    os.system("txt2xls -b %s > %s" % (sq(TXT_FILE), sq(summary_file)))
    filelib.assert_exists_nz(summary_file)
    os.unlink(TXT_FILE)


def _make_histo_file(cov_filename, outfilename):
    from genomicode import ngslib

    results = ngslib.parse_bedtools_genomecov_results(
        cov_filename)

    # [0, 1), [1, 5), [5, 10), etc.
    BINS = [0, 1, 5, 10, 25, 50, 100, 250, 500, 1000, 5000, 10000]
    counts = [0] * len(BINS)

    for x in results:
        chrom, coverage, num_bases, chrom_size, perc_bases = x
        if chrom != "genome":
            continue
        bin_i = None
        for i in range(len(BINS)-1):
            b1, b2 = BINS[i], BINS[i+1]
            if coverage >= b1 and coverage < b2:
                bin_i = i
                break
        if bin_i is None:
            assert coverage >= BINS[-1]
            bin_i = len(BINS)-1
        counts[bin_i] += num_bases

    total = sum(counts)
    perc = [float(x)/total for x in counts]

    total_noz = sum(counts[1:])
    perc_noz = [float(x)/total_noz for x in counts]
    perc_noz[0] = 0

    total_10 = sum(counts[3:])
    perc_10 = [float(x)/total_10 for x in counts]
    perc_10[0] = perc_10[1] = perc_10[2] = 0

    handle = open(outfilename, 'w')
    header = "Range", "Count", "Fraction", "Fraction (>= 1)", \
             "Fraction (>= 10)"
    print >>handle, "\t".join(header)
    for i in range(len(BINS)):
        if i < len(BINS)-1:
            b1, b2 = BINS[i], BINS[i+1]
            assert b2 > b1
            if b2 == b1+1:
                r = b1
            else:
                r = "%d-%d" % (b1, b2-1)
        else:
            r = ">= %d" % BINS[i]
        x = r, counts[i], perc[i], perc_noz[i], perc_10[i]
        assert len(x) == len(header)
        print >>handle, "\t".join(map(str, x))
    handle.close()


def _make_intervallist_file(intervallist_file, features_bed, bam_filename):
    from genomicode import config
    from genomicode import filelib
    from genomicode import parallel

    outhandle = open(intervallist_file, 'w')

    # Add the @HD and @SQ headers from the bam file.
    # samtools view -H <filename>
    samtools = filelib.which_assert(config.samtools)
    sq = parallel.quote
    cmd = [
        sq(samtools),
        "view",
        "-H",
        sq(bam_filename),
        ]
    cmd = " ".join(cmd)
    x = parallel.sshell(cmd)
    lines = x.split("\n")
    lines = [x.rstrip() for x in lines]

    for line in lines:
        if line.startswith("@HD") or line.startswith("@SQ"):
            print >>outhandle, line

    # Add the information from the BAM files.
    # BED       chrom chromStart (0-based) chromEnd name score strand
    # Interval  chrom chromStart (1-based) chromEnd strand name
    for cols in filelib.read_cols(features_bed):
        assert len(cols) >= 6
        chrom, chromStart0, chromEnd, name, score, strand = cols[:6]
        chromStart0, chromEnd = int(chromStart0), int(chromEnd)
        chromStart1 = chromStart0 + 1
        x = chrom, chromStart1, chromEnd, strand, name
        print >>outhandle, "\t".join(map(str, x))
    outhandle.close()


def _make_calculatehsmetrics_command(
    intervallist_file, bam_filename, cov_filename, targetcov_filename,
    reference_file, log_filename):
    from genomicode import alignlib
    from genomicode import parallel

    # java -Xmx10g -jar /usr/local/bin/picard/CalculateHsMetrics.jar \
    #      BAIT_INTERVALS=exon${j}3.list \
    #      TARGET_INTERVALS=exon${j}3.list \
    #      INPUT=samp${i}3.bam \
    #      OUTPUT=cov${i}1.txt \
    #      PER_TARGET_COVERAGE=cov${i}2.txt \
    #      REFERENCE_SEQUENCE=ref01.fa >& cov${i}3.log

    sq = parallel.quote
    picard_jar = alignlib.find_picard_jar("picard")
    cmd = [
        "java",
        "-Xmx10g",
        "-jar", sq(picard_jar), "CalculateHsMetrics",
        "BAIT_INTERVALS=%s" % sq(intervallist_file),
        "TARGET_INTERVALS=%s" % sq(intervallist_file),
        "INPUT=%s" % sq(bam_filename),
        "OUTPUT=%s" % sq(cov_filename),
        "PER_TARGET_COVERAGE=%s" % sq(targetcov_filename),
        "REFERENCE_SEQUENCE=%s" % sq(reference_file),
        ]
    cmd = " ".join(cmd)
    cmd = "%s >& %s" % (cmd, sq(log_filename))
    return cmd


def _read_coverage_file(filename):
    # Return dict of key -> value (as string)
    from genomicode import filelib

    matrix = []
    for line in filelib.openfh(filename):
        if line.startswith("#"):
            continue
        if not line.strip():
            continue
        x = line.rstrip("\r\n").split("\t")
        matrix.append(x)
    assert len(matrix) == 2
    assert len(matrix[0]) == len(matrix[1])

    data = {}
    for i in range(len(matrix[0])):
        key = matrix[0][i].strip()
        value = matrix[1][i].strip()
        assert key not in data
        data[key] = value
    return data


def _make_samtools_filter_cmd(in_bamfile, out_bamfile):
    from genomicode import filelib
    from genomicode import parallel
    from genomicode import config

    filelib.assert_exists_nz(in_bamfile)
    samtools = filelib.which_assert(config.samtools)
    sq = parallel.quote

    cmd = [
        sq(samtools),
        "view",
        "-bF 4",
        sq(in_bamfile),
        ">",
        sq(out_bamfile),
        ]
    cmd = " ".join(cmd)
    return cmd
